{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1",
   "metadata": {},
   "source": [
    "# 01 — Baseline Prompting\n",
    "\n",
    "Shows how **Qwen2.5-3B-Instruct** performs on the letter-counting task\n",
    "**without** any fine-tuning, using a Chain-of-Thought system prompt.\n",
    "\n",
    "Task: *\"How many of the letter 'g' are there in the word 'engage'?\"*\n",
    "\n",
    "The model should list each letter with a running count inside `<reasoning>` tags,\n",
    "then give the final answer inside `<answer>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup — install deps if needed (e.g. on Colab)\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import trl\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "        \"torch\", \"transformers>=4.40\", \"datasets\", \"accelerate\",\n",
    "        \"peft>=0.10\", \"trl>=0.15\", \"pandas\", \"matplotlib\"])\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prompting_baseline import load_base_model, generate\n",
    "from src.rewards import reward_single\n",
    "\n",
    "model, tokenizer = load_base_model()\n",
    "print(f\"Model loaded on: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4",
   "metadata": {},
   "source": [
    "## Baseline Results\n",
    "\n",
    "Run the baseline model on three test cases and score with our reward functions.\n",
    "The model typically struggles to count specific letter occurrences accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [(\"engage\", \"g\"), (\"banana\", \"a\"), (\"strawberry\", \"r\")]\n",
    "\n",
    "for word, letter in test_cases:\n",
    "    true_count = word.count(letter)\n",
    "    response = generate(model, tokenizer, word, letter)\n",
    "    rewards = reward_single(word, letter, true_count, response)\n",
    "\n",
    "    print(f'\\n{\"=\"*55}')\n",
    "    print(f'How many \"{letter}\" in \"{word}\"?  (true count: {true_count})')\n",
    "    print(f'{\"=\"*55}')\n",
    "    print(response)\n",
    "    print(f\"\\nReward breakdown: {rewards}\")\n",
    "    print(f\"Total reward: {rewards.total:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "The pretrained model often:\n",
    "- Miscounts the running total of the target letter\n",
    "- Gives the wrong final answer\n",
    "- May not follow the exact `<reasoning>/<answer>` format\n",
    "\n",
    "This motivates GRPO fine-tuning to teach the model accurate letter counting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
