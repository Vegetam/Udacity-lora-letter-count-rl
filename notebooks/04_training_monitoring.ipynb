{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1",
   "metadata": {},
   "source": [
    "# 04 — GRPO Training & Monitoring\n",
    "\n",
    "Runs **Group Relative Policy Optimization (GRPO)** training on\n",
    "Qwen2.5-3B-Instruct with a LoRA adapter.\n",
    "\n",
    "**Requirements**: GPU with >= 12 GB VRAM (e.g. Colab T4 / A100).\n",
    "If running on Colab, make sure to select **Runtime → Change runtime type → GPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup — install deps if needed (e.g. on Colab)\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import trl\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "        \"torch\", \"transformers>=4.40\", \"datasets\", \"accelerate\",\n",
    "        \"peft>=0.10\", \"trl>=0.15\", \"pandas\", \"matplotlib\"])\n",
    "\n",
    "import os, sys, torch\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3",
   "metadata": {},
   "source": [
    "## Quick Sanity Check (5 steps)\n",
    "\n",
    "Run a short training pass to confirm reward functions produce non-zero values\n",
    "and training doesn't crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import train\n",
    "\n",
    "# Quick check — 5 steps only\n",
    "train(max_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5",
   "metadata": {},
   "source": [
    "## Full Training (100 steps)\n",
    "\n",
    "Run the full training session. The `correct_answer_reward_func` mean\n",
    "should trend **upward** over time as the model learns to count letters accurately.\n",
    "\n",
    "Expected time: ~15–30 min on a T4, ~5–10 min on an A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(max_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7",
   "metadata": {},
   "source": [
    "## Plot Training Rewards\n",
    "\n",
    "Visualise the mean correctness reward over training steps.\n",
    "A clear upward trend confirms that GRPO is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csv_path = \"../outputs/logs/mean_correctness_over_time.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Reward log (last 10 rows):\")\n",
    "print(df.tail(10).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(df[\"step\"], df[\"mean_correctness_reward\"], marker=\"o\", linewidth=2)\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Mean Correctness Reward\")\n",
    "plt.title(\"Correctness Reward Over GRPO Training\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
