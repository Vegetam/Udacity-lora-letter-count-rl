{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1",
   "metadata": {},
   "source": [
    "# 02 â€” Reward Design & Validation\n",
    "\n",
    "Validates that the five reward functions give **higher scores** for correct\n",
    "samples and **lower scores** for incorrect samples.\n",
    "\n",
    "Reward functions:\n",
    "| Function | What it checks | Max |\n",
    "|---|---|---|\n",
    "| `format_reward_func` | `<reasoning>/<answer>` tags + digit answer | 1.0 |\n",
    "| `numbering_reward_func` | Sequential line numbers (1, 2, 3, ...) | ~1.0 |\n",
    "| `spelling_reward_func` | Listed letters match the word | 2.0 |\n",
    "| `counting_reward_func` | Running count of target letter is accurate | ~1.0 |\n",
    "| `correct_answer_reward_func` | Final answer matches true count | 2.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from src.utils import read_jsonl\n",
    "from src.rewards import reward_single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3",
   "metadata": {},
   "source": [
    "## Correct Samples (expect HIGH rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = read_jsonl(\"../data/samples_correct.jsonl\")\n",
    "\n",
    "for row in correct:\n",
    "    r = reward_single(row[\"word\"], row[\"letter\"], row[\"count\"], row[\"completion\"])\n",
    "    print(f'\\n{\"=\"*55}')\n",
    "    print(f'  \"{row[\"letter\"]}\" in \"{row[\"word\"]}\"  (true count: {row[\"count\"]})')\n",
    "    print(f'{\"=\"*55}')\n",
    "    print(f\"  Response:\\n{row['completion']}\")\n",
    "    print(f\"\\n  Rewards: {r}\")\n",
    "    print(f\"  Total:   {r.total:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5",
   "metadata": {},
   "source": [
    "## Incorrect Samples (expect LOW rewards)\n",
    "\n",
    "These have errors: wrong running counts, skipped letters, missing format tags, wrong answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = read_jsonl(\"../data/samples_incorrect.jsonl\")\n",
    "\n",
    "for row in incorrect:\n",
    "    r = reward_single(row[\"word\"], row[\"letter\"], row[\"count\"], row[\"completion\"])\n",
    "    print(f'\\n{\"=\"*55}')\n",
    "    print(f'  \"{row[\"letter\"]}\" in \"{row[\"word\"]}\"  (true count: {row[\"count\"]})')\n",
    "    print(f'{\"=\"*55}')\n",
    "    print(f\"  Response:\\n{row['completion']}\")\n",
    "    print(f\"\\n  Rewards: {r}\")\n",
    "    print(f\"  Total:   {r.total:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Correct samples consistently score **much higher** than incorrect ones,\n",
    "confirming that the reward functions can guide GRPO training effectively.\n",
    "\n",
    "Key failure modes penalised:\n",
    "- Wrong final answer (loses 2.0 from `correct_answer_reward_func`)\n",
    "- Wrong running counts (loses from `counting_reward_func`)\n",
    "- Skipped/extra letters (loses from `spelling_reward_func` and `numbering_reward_func`)\n",
    "- Missing `<reasoning>/<answer>` tags (loses from `format_reward_func`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
