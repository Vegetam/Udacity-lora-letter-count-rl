{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1",
   "metadata": {},
   "source": [
    "# 03 — LoRA Model Setup\n",
    "\n",
    "Demonstrates the LoRA (Low-Rank Adaptation) configuration used for fine-tuning\n",
    "**Qwen2.5-3B-Instruct**.\n",
    "\n",
    "### Configuration Choices\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|---|---|---|\n",
    "| `lora_rank` | 32 | Good capacity while staying memory-friendly |\n",
    "| `lora_alpha` | 64 | 2 × rank — standard heuristic for stable learning |\n",
    "| `lora_dropout` | 0.1 | Light regularisation to prevent overfitting |\n",
    "| `target_modules` | q/k/v/o_proj + gate/up/down_proj | All key linear layers in attention + MLP |\n",
    "| `use_4bit` | True | QLoRA — reduces ~6 GB (fp16) to ~2 GB (4-bit) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import peft\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "        \"torch\", \"transformers>=4.40\", \"accelerate\",\n",
    "        \"peft>=0.10\", \"bitsandbytes>=0.43\"])\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lora_setup import LoraSetup, build_trainable_lora_model\n",
    "\n",
    "cfg = LoraSetup(\n",
    "    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    lora_rank=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    use_4bit=True,\n",
    ")\n",
    "\n",
    "print(\"LoRA configuration:\")\n",
    "print(f\"  Model:          {cfg.model_name}\")\n",
    "print(f\"  Rank:           {cfg.lora_rank}\")\n",
    "print(f\"  Alpha:          {cfg.lora_alpha}\")\n",
    "print(f\"  Dropout:        {cfg.lora_dropout}\")\n",
    "print(f\"  Target modules: {cfg.target_modules_requested}\")\n",
    "print(f\"  4-bit quant:    {cfg.use_4bit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, lora_config = build_trainable_lora_model(cfg)\n",
    "\n",
    "print(f\"\\nModel device: {model.device}\")\n",
    "print(f\"\\nLoRA config object:\\n{lora_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5",
   "metadata": {},
   "source": [
    "### Why these choices?\n",
    "\n",
    "- **Rank 32**: Good middle ground — rank 8/16 may lack capacity for learning\n",
    "  the letter-counting reasoning pattern; rank 64/128 uses more memory with\n",
    "  diminishing returns for this relatively simple task.\n",
    "- **All 7 target modules**: Targeting both attention *and* MLP layers gives\n",
    "  the adapter maximum expressiveness to learn the counting behaviour.\n",
    "- **4-bit quantization**: Reduces the frozen base model from ~6 GB to ~2 GB,\n",
    "  making it feasible to train on GPUs with 8–16 GB VRAM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
