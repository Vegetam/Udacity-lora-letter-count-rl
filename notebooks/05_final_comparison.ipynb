{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1",
   "metadata": {},
   "source": [
    "# 05 â€” Final Comparison (Base vs Fine-tuned)\n",
    "\n",
    "Compare the **pretrained** Qwen2.5-3B-Instruct (no training) with the\n",
    "**GRPO fine-tuned** model on the letter-counting task.\n",
    "\n",
    "Also test for **catastrophic forgetting** using a general-knowledge question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import subprocess, sys\n",
    "try:\n",
    "    import trl\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "        \"torch\", \"transformers>=4.40\", \"datasets\", \"accelerate\",\n",
    "        \"peft>=0.10\", \"trl>=0.15\", \"pandas\", \"matplotlib\"])\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3",
   "metadata": {},
   "source": [
    "## Letter-Counting Comparison\n",
    "\n",
    "Compare base model vs fine-tuned model on a word from the training set\n",
    "and reward-score both responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import compare_models, compare_general_knowledge\n",
    "\n",
    "# Letter-counting comparison\n",
    "compare_models(\"engage\", \"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another word\n",
    "compare_models(\"strawberry\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6",
   "metadata": {},
   "source": [
    "## Catastrophic Forgetting Check\n",
    "\n",
    "Ask both models a general-knowledge question to verify that fine-tuning\n",
    "did not erase the model's existing knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_general_knowledge(\"What is the capital of the Philippines?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- The **fine-tuned model** should show significantly higher reward scores on\n",
    "  the letter-counting task, especially on `correct_answer_reward_func`.\n",
    "- Both models should answer the general-knowledge question correctly,\n",
    "  confirming **no catastrophic forgetting** occurred during LoRA fine-tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
